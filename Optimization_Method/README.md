# Optimization Methods

## Table Content
- Introduction
- Algorithms
    - Gradient Descent
- Challenges
* Reference

## Introduction 
**Optimization** is a group of methods used for finding a global or local minimum of a cost function. It's important in the field of machine learning because it provides an iteractive way to update weights for most all models.


## Algorithms 

### Gradient Descent
Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. Stochastic gradient descent and mini-batch gradient descent are two variants of (batch) gradient descent.


## Challenges
These problems include taking too long to train, vanishing and exploding gradients and initialization. 
* Taking lots of time in training process.
* Vanishing and exploding gradients.
* Cannot guarantee global minimum, but usually local minimum.
* Result is highly dependent on initialization.


## Reference
1.Wikimedia Foundation. (2021, November 2). Gradient descent. Wikipedia. Retrieved December 13, 2021, from https://en.wikipedia.org/wiki/Gradient_descent. 
